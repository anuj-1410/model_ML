{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMERA_INDEX=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import seaborn as sns\n",
    "import collections  # New import for prediction smoothing\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CREATE DATASET FUNCTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CREATE DATASET FUNCTIONS\n",
    "def collect_hand_landmarks_dataset(num_samples_per_digit=100):\n",
    "    \"\"\"Collect hand landmarks data for digits 0-9\"\"\"\n",
    "    # Create directory for the dataset if it doesn't exist\n",
    "    dataset_dir = 'hand_landmarks_dataset'\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "    \n",
    "    landmarks_data = []\n",
    "    labels = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(CAMERA_INDEX)  # Use 0 for default camera, or 1 for external webcam\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "    \n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=0.7) as hands:\n",
    "        \n",
    "        # Collect data for each digit (0-9)\n",
    "        for digit in range(10):\n",
    "            print(f\"\\nShow digit {digit} with your hand.\")\n",
    "            print(f\"Collecting {num_samples_per_digit} samples for digit {digit}...\")\n",
    "            print(\"Press 's' to start collecting, 'q' to quit\")\n",
    "            \n",
    "            samples_collected = 0\n",
    "            collecting = False\n",
    "            \n",
    "            while samples_collected < num_samples_per_digit:\n",
    "                # Read frame from webcam\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Error: Could not read frame.\")\n",
    "                    break\n",
    "                \n",
    "                # Flip the frame horizontally for a more intuitive mirror view\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Convert to RGB for MediaPipe\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process the frame to detect hands\n",
    "                results = hands.process(rgb_frame)\n",
    "                \n",
    "                # Display instructions\n",
    "                instructions = f\"Digit: {digit} | Samples: {samples_collected}/{num_samples_per_digit}\"\n",
    "                cv2.putText(frame, instructions, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                if collecting:\n",
    "                    cv2.putText(frame, \"COLLECTING...\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                else:\n",
    "                    cv2.putText(frame, \"Press 's' to start collecting\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "                \n",
    "                # Draw hand landmarks if detected\n",
    "                if results.multi_hand_landmarks:\n",
    "                    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            frame, \n",
    "                            hand_landmarks, \n",
    "                            mp_hands.HAND_CONNECTIONS,\n",
    "                            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                            mp_drawing_styles.get_default_hand_connections_style()\n",
    "                        )\n",
    "                        \n",
    "                        # If collecting and hand is detected, save landmarks\n",
    "                        if collecting:\n",
    "                            # Extract landmark coordinates (x, y, z) for all 21 hand landmarks\n",
    "                            landmarks = []\n",
    "                            for landmark in hand_landmarks.landmark:\n",
    "                                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "                            \n",
    "                            # Add to dataset\n",
    "                            landmarks_data.append(landmarks)\n",
    "                            labels.append(digit)\n",
    "                            samples_collected += 1\n",
    "                            \n",
    "                            # Show progress\n",
    "                            print(f\"\\rCollected {samples_collected}/{num_samples_per_digit} samples for digit {digit}\", end=\"\")\n",
    "                \n",
    "                # Display the frame\n",
    "                cv2.imshow('Hand Gesture Dataset Collection', frame)\n",
    "                \n",
    "                # Handle key presses\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    collecting = False\n",
    "                    break\n",
    "                elif key == ord('s'):\n",
    "                    collecting = True\n",
    "            \n",
    "            print(f\"\\nFinished collecting samples for digit {digit}\")\n",
    "            \n",
    "            # If user quit, break out of the digit loop\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(landmarks_data)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Save the dataset\n",
    "    with open(os.path.join(dataset_dir, 'hand_landmarks_dataset.pkl'), 'wb') as f:\n",
    "        pickle.dump({'features': X, 'labels': y}, f)\n",
    "    \n",
    "    print(\"\\nDataset collection completed!\")\n",
    "    print(f\"Total samples collected: {len(X)}\")\n",
    "    return X, y\n",
    "\n",
    "def load_dataset(filepath='hand_landmarks_dataset/hand_landmarks_dataset.pkl'):\n",
    "    \"\"\"Load the hand landmarks dataset\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(\"Dataset not found. Please collect data first.\")\n",
    "        return None, None\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    return data['features'], data['labels']\n",
    "\n",
    "def collect_additional_samples(digits_to_improve=[5, 6, 7, 8, 9], samples_per_digit=50):\n",
    "    \"\"\"Collect additional samples for specific digits that need improvement\"\"\"\n",
    "    # Load existing dataset\n",
    "    dataset_path = 'hand_landmarks_dataset/hand_landmarks_dataset.pkl'\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    existing_features = data['features']\n",
    "    existing_labels = data['labels']\n",
    "    \n",
    "    # Initialize data collection variables\n",
    "    new_landmarks = []\n",
    "    new_labels = []\n",
    "    \n",
    "    # Initialize the webcam\n",
    "    cap = cv2.VideoCapture(1)  # Use 0 for default camera, or 1 for external webcam\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize MediaPipe Hands\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=False,\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=0.7) as hands:\n",
    "        \n",
    "        # Collect data for each digit to improve\n",
    "        for digit in digits_to_improve:\n",
    "            print(f\"\\nShow digit {digit} with your hand.\")\n",
    "            print(f\"Collecting {samples_per_digit} additional samples...\")\n",
    "            print(\"Try different angles and distances from the camera\")\n",
    "            print(\"Press 's' to start collecting, 'q' to quit\")\n",
    "            \n",
    "            samples_collected = 0\n",
    "            collecting = False\n",
    "            \n",
    "            while samples_collected < samples_per_digit:\n",
    "                # Read frame from webcam\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Flip the frame horizontally\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Convert to RGB for MediaPipe\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Process the frame\n",
    "                results = hands.process(rgb_frame)\n",
    "                \n",
    "                # Display instructions\n",
    "                cv2.putText(frame, f\"Digit: {digit} | Samples: {samples_collected}/{samples_per_digit}\", \n",
    "                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "                if collecting:\n",
    "                    cv2.putText(frame, \"COLLECTING...\", (10, 60), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                else:\n",
    "                    cv2.putText(frame, \"Press 's' to start collecting\", (10, 60), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "                \n",
    "                # Draw hand landmarks\n",
    "                if results.multi_hand_landmarks:\n",
    "                    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                            mp_drawing_styles.get_default_hand_connections_style()\n",
    "                        )\n",
    "                        \n",
    "                        # If collecting and hand is detected, save landmarks\n",
    "                        if collecting:\n",
    "                            landmarks = []\n",
    "                            for landmark in hand_landmarks.landmark:\n",
    "                                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "                            \n",
    "                            # Add to dataset\n",
    "                            new_landmarks.append(landmarks)\n",
    "                            new_labels.append(digit)\n",
    "                            samples_collected += 1\n",
    "                            \n",
    "                            print(f\"\\rCollected {samples_collected}/{samples_per_digit} samples for digit {digit}\", end=\"\")\n",
    "                \n",
    "                # Display the frame\n",
    "                cv2.imshow('Additional Data Collection', frame)\n",
    "                \n",
    "                # Handle key presses\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    collecting = False\n",
    "                    break\n",
    "                elif key == ord('s'):\n",
    "                    collecting = True\n",
    "            \n",
    "            print(f\"\\nFinished collecting additional samples for digit {digit}\")\n",
    "            \n",
    "            if key == ord('q'):\n",
    "                break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Combine with existing dataset\n",
    "    combined_features = np.vstack([existing_features, np.array(new_landmarks)])\n",
    "    combined_labels = np.append(existing_labels, np.array(new_labels))\n",
    "    \n",
    "    # Save the updated dataset\n",
    "    with open(dataset_path, 'wb') as f:\n",
    "        pickle.dump({'features': combined_features, 'labels': combined_labels}, f)\n",
    "    \n",
    "    print(\"\\nDataset updated successfully!\")\n",
    "    print(f\"Added {len(new_landmarks)} new samples\")\n",
    "    print(f\"Total samples now: {len(combined_features)}\")\n",
    "    \n",
    "    return combined_features, combined_labels\n",
    "\n",
    "def collect_hand_gesture_images(num_samples_per_class=100):\n",
    "    # Create a folder for the dataset if needed\n",
    "    dataset_dir = 'hand_gesture_images'\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "    \n",
    "    cap = cv2.VideoCapture(1)\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Loop over each class (digit 0-9)\n",
    "    for digit in range(10):\n",
    "        print(f\"Collecting images for digit {digit}. Press 's' to start and 'q' to quit.\")\n",
    "        samples_collected = 0\n",
    "        while samples_collected < num_samples_per_class:\n",
    "            ret, img = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            # Optional: flip/mirror depending on desired view\n",
    "            img = cv2.flip(img, 1)\n",
    "            # Preprocess the image (get the 64x64 grayscale image)\n",
    "            # img_final, _ = preprocess_image(img)\n",
    "            # Remove the batch dimension -> shape becomes (64,64,1)\n",
    "            # img_final = img_final[0]\n",
    "            \n",
    "            cv2.putText(img, f\"{digit}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow(\"Collect\", img)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('s'):  # start collecting\n",
    "                # all_images.append(img_final)\n",
    "                all_labels.append(digit)\n",
    "                samples_collected += 1\n",
    "                print(f\"Collected {samples_collected}/{num_samples_per_class} for digit {digit}\")\n",
    "            elif key == ord('q'):\n",
    "                break\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return np.array(all_images), np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CREATE MODEL FOR HAND GESTURE RECOGNITION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hand_gesture_model(input_dim, num_classes=10):\n",
    "    \"\"\"Create a neural network for hand gesture recognition based on landmarks\"\"\"\n",
    "    \n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # First dense layer\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Second dense layer\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_improved_hand_gesture_model(input_dim, num_classes=10):\n",
    "    \"\"\"Create an improved model with more capacity to distinguish similar gestures\"\"\"\n",
    "    \n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # First dense layer - bigger\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Second dense layer\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Third dense layer\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    # Compile with a lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_complex_hand_gesture_model(input_dim, num_classes=10):\n",
    "    \"\"\"Create a more complex and regularized neural network for hand gesture recognition with fine tuning\"\"\"\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    \n",
    "    # First dense block\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)  # Reduced dropout for fine tuning\n",
    "    \n",
    "    # Second dense block\n",
    "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)  # Reduced dropout\n",
    "    \n",
    "    # Third dense block\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Fourth dense block\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.00025),  # Lowered learning rate\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_cnn_hand_gesture_model(input_shape=(64, 64, 1), num_classes=10):\n",
    "    \"\"\"\n",
    "    Create a CNN model for hand gesture recognition.\n",
    "    Input: Grayscale image of size 64x64 (shape=(64,64,1)).\n",
    "    Output: 10-class softmax predictions.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # First convolutional block\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Second block\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Third block\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Fully connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TRAIN MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hand_gesture_model(X, y):\n",
    "    \"\"\"Train the hand gesture recognition model\"\"\"\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Split training data to create validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_val = to_categorical(y_val, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    \n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Test samples: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Create model\n",
    "    input_dim = X_train.shape[1]  # Number of features (21 landmarks × 3 coordinates)\n",
    "    model = create_hand_gesture_model(input_dim, num_classes=10)\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_hand_gesture_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,  # More epochs with early stopping\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hand_gesture_training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def normalize_landmarks(landmarks):\n",
    "    \"\"\"Normalize hand landmarks to be invariant to scale and translation\"\"\"\n",
    "    # Reshape landmarks array to separate x, y, z coordinates\n",
    "    landmarks_array = np.array(landmarks).reshape(-1, 3)\n",
    "    \n",
    "    # Get wrist position (typically the first landmark)\n",
    "    wrist = landmarks_array[0]\n",
    "    \n",
    "    # Center landmarks around wrist\n",
    "    centered = landmarks_array - wrist\n",
    "    \n",
    "    # Find the scale (maximum distance from any landmark to wrist)\n",
    "    scale = np.max(np.linalg.norm(centered, axis=1))\n",
    "    if scale > 0:\n",
    "        # Normalize by scale\n",
    "        normalized = centered / scale\n",
    "    else:\n",
    "        normalized = centered\n",
    "    \n",
    "    # Flatten back to original shape\n",
    "    return normalized.flatten()\n",
    "\n",
    "def augment_landmarks(landmarks, noise_std=0.01, num_augments=3):\n",
    "    \"\"\"\n",
    "    Generate augmented copies of a landmark sample by adding Gaussian noise.\n",
    "    noise_std: standard deviation of the noise (adjust based on normalized scale)\n",
    "    num_augments: number of augmented copies per original sample\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    landmarks_array = np.array(landmarks)\n",
    "    for _ in range(num_augments):\n",
    "        noise = np.random.normal(0, noise_std, landmarks_array.shape)\n",
    "        augmented.append(landmarks_array + noise)\n",
    "    return augmented\n",
    "\n",
    "def train_with_class_weighting(X, y):\n",
    "    \"\"\"Train the model using a more complex model with class weighting and data augmentation.\"\"\"\n",
    "    # Normalize each training sample using the same normalize_landmarks function\n",
    "    X_norm = np.array([normalize_landmarks(sample) for sample in X])\n",
    "    \n",
    "    # Augment the dataset – for every sample, add a few jittered copies\n",
    "    X_aug_list = []\n",
    "    y_aug_list = []\n",
    "    for i, sample in enumerate(X_norm):\n",
    "        aug_samples = augment_landmarks(sample, noise_std=0.01, num_augments=3)\n",
    "        X_aug_list.extend(aug_samples)\n",
    "        y_aug_list.extend([y[i]] * len(aug_samples))\n",
    "    \n",
    "    print(f\"Original samples: {len(X_norm)}\")\n",
    "    print(f\"Augmented samples added: {len(X_aug_list)}\")\n",
    "    \n",
    "    # Combine original and augmented data\n",
    "    X_total = np.concatenate((X_norm, np.array(X_aug_list)), axis=0)\n",
    "    y_total = np.concatenate((y, np.array(y_aug_list)), axis=0)\n",
    "    \n",
    "    # Split data into train, validation and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_train_cat = to_categorical(y_train, 10)\n",
    "    y_val_cat = to_categorical(y_val, 10)\n",
    "    y_test_cat = to_categorical(y_test, 10)\n",
    "    \n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Test samples: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Define class weights (to focus on problematic digits)\n",
    "    class_weights = {\n",
    "        0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0,\n",
    "        5: 2.0, 6: 2.0, 7: 2.0, 8: 2.0, 9: 2.0\n",
    "    }\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    # Use your fine-tuned complex model\n",
    "    model = create_complex_hand_gesture_model(input_dim, num_classes=10)\n",
    "    \n",
    "    # Use callbacks adjusted for fine tuning\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint('best_hand_gesture_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, min_lr=0.000001, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        epochs=500,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_cat)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Calculate accuracy using sklearn's function\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    true_classes = np.argmax(y_test_cat, axis=1)\n",
    "    acc = accuracy_score(true_classes, y_pred_classes)\n",
    "    print(\"Sklearn Accuracy Score:\", acc)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. REAL-TIME RECOGNITION WITH MEDIAPIPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_hand_gesture_recognition(model=None):\n",
    "    \"\"\"Real-time hand gesture recognition with prediction smoothing using MediaPipe\"\"\"\n",
    "    if model is None:\n",
    "        if os.path.exists('best_hand_gesture_model.h5'):\n",
    "            model = load_model('4best_hand_gesture_model.h5')\n",
    "        else:\n",
    "            print(\"Model not found. Please train the model first.\")\n",
    "            return\n",
    "\n",
    "    print(\"Starting real-time recognition. Press 'q' to exit.\")\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    # Buffer for smoothing predictions (last 5 predictions)\n",
    "    prediction_buffer = collections.deque(maxlen=5)\n",
    "\n",
    "    # Initialize MediaPipe Hands (already declared at top)\n",
    "    with mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7) as hands:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Could not read frame.\")\n",
    "                break\n",
    "            \n",
    "            # Flip for a mirror view (optional)\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(rgb_frame)\n",
    "\n",
    "            if results.multi_hand_landmarks and results.multi_handedness:\n",
    "                # Process each detected hand along with its handedness info.\n",
    "                for hand_landmarks, hand_info in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                    # Extract landmarks from MediaPipe (already normalized to [0,1])\n",
    "                    landmarks = [[lmk.x, lmk.y, lmk.z] for lmk in hand_landmarks.landmark]\n",
    "                    \n",
    "                    # Mirror right-hand landmarks: MediaPipe returns landmarks in normalized coordinates.\n",
    "                    # When the hand is \"Right\", we mirror by replacing x with (1 - x).\n",
    "                    hand_label = hand_info.classification[0].label   # \"Left\" or \"Right\"\n",
    "                    if hand_label == \"Right\":\n",
    "                        landmarks = [[1 - pt[0], pt[1], pt[2]] for pt in landmarks]\n",
    "                    \n",
    "                    # Flatten and then normalize landmarks so they are translation and scale invariant.\n",
    "                    # (normalize_landmarks expects a flat array or an array reshaped to (-1,3))\n",
    "                    landmarks_flat = np.array(landmarks).flatten()\n",
    "                    landmarks_normalized = normalize_landmarks(landmarks_flat)\n",
    "                    \n",
    "                    # Predict the gesture using the model\n",
    "                    prediction = model.predict(np.array([landmarks_normalized]), verbose=0)\n",
    "                    index = np.argmax(prediction[0])\n",
    "                    confidence = prediction[0][index]\n",
    "                    \n",
    "                    # Append to prediction buffer for smoothing\n",
    "                    prediction_buffer.append(index)\n",
    "                    smoothed_pred = max(set(prediction_buffer), key=prediction_buffer.count)\n",
    "                    \n",
    "                    # Draw prediction on the frame\n",
    "                    cv2.putText(frame, f\"{smoothed_pred} ({confidence:.2f})\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    # Optionally, you can also draw the landmarks using mp_drawing.\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                        mp_drawing_styles.get_default_hand_connections_style()\n",
    "                    )\n",
    "\n",
    "            cv2.imshow(\"Real-Time Hand Gesture Recognition\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def evaluate_with_confusion_matrix(model, X, y):\n",
    "    \"\"\"Evaluate model and show confusion matrix to identify problem digits\"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y, y_pred_classes)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MAIN EXECUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time recognition. Press 'q' to exit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 15:38:12.455 python[55721:2481058] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743934093.797386 2481058 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2025-04-06 15:38:14.391 python[55721:2481058] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-06 15:38:14.391 python[55721:2481058] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "2025-04-06 15:38:16.449030: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m y_test_cat \u001b[38;5;241m=\u001b[39m to_categorical(y_test, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Evaluate the model using the evaluate function (returns loss and accuracy)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test_cat, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Evaluate Test Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Alternatively, calculate accuracy manually:\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Check if dataset exists\n",
    "    dataset_path = 'hand_landmarks_dataset/hand_landmarks_dataset.pkl'\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"Dataset not found. Starting data collection...\")\n",
    "        X, y = collect_hand_landmarks_dataset(num_samples_per_digit=10)\n",
    "    else:\n",
    "        # Ask if user wants to collect new data or use existing\n",
    "        choice = input(\"Dataset found. Do you want to [t]rain on existing data, [c]ollect new data, or [r]un recognition? (t/c/r): \").lower()\n",
    "        \n",
    "        if choice == 'c':\n",
    "            X, y = collect_hand_gesture_images(num_samples_per_class=10)\n",
    "        elif choice == 't':\n",
    "            X, y = load_dataset(dataset_path)\n",
    "            if X is not None and y is not None:\n",
    "                model = train_with_class_weighting(X, y)\n",
    "        elif choice == 'r':\n",
    "            real_time_hand_gesture_recognition()\n",
    "        else:\n",
    "            print(\"Invalid choice. Exiting.\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Updated Evaluation Code:\n",
    "    # -------------------------------\n",
    "    \n",
    "    # Load dataset previously collected (adjust the file path as needed)\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # X_raw contains raw landmark features as collected (length 63 per sample)\n",
    "    X_raw = np.array(data['features'])\n",
    "    # IMPORTANT: Normalize each sample using the same function as used during training\n",
    "    X = np.array([normalize_landmarks(sample) for sample in X_raw])\n",
    "    y = np.array(data['labels'])      # Labels remain as integers (0-9)\n",
    "\n",
    "    # Split data into training and testing sets (80-20 split)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert labels to one-hot vectors for evaluation if using categorical_crossentropy\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    y_test_cat = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "    # Evaluate the model using the evaluate function (returns loss and accuracy)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    print(\"Model Evaluate Test Accuracy:\", accuracy)\n",
    "\n",
    "    # Alternatively, calculate accuracy manually:\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    acc = accuracy_score(y_test, predicted_classes)\n",
    "    print(\"Sklearn Accuracy Score:\", acc)\n",
    "\n",
    "    # Example training code:\n",
    "    X, y = collect_hand_gesture_images(num_samples_per_class=50)  # Adjust sample count as needed\n",
    "    X = X.astype('float32')  # Already normalized by preprocess_image (divided by 255)\n",
    "    \n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "    y_test_cat = to_categorical(y_test, num_classes=10)\n",
    "    \n",
    "    # Create CNN model\n",
    "    model = create_cnn_hand_gesture_model(input_shape=(64,64,1), num_classes=10)\n",
    "    \n",
    "    # Train the CNN\n",
    "    history = model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        epochs=50,  # adjust epochs as needed\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "    print(\"CNN Test Accuracy:\", test_acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
